<html>
<head>
<title>Shaodi YOU - Publications</title>
<meta http-equiv="Content-Type" content="text/html; charset=Helvetica">

<style type="text/css">
p {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 16px;
}
.STYLE3 {font-size: 24px}
</style>
</head>
<body bgcolor="#FFFFFF" leftmargin="0" topmargin="0" marginwidth="0" marginheight="0">
<!-- Save for Web Slices (Untitled-1.psd) -->
<table width="840" height="768" border="0" align="center" cellpadding="0" cellspacing="0" id="Table_01">
	<tr>
		<td colspan="6">
			<img src="../images/index_01.gif" width="840" height="195" alt=""></td>
	</tr>
	<tr>
		<td rowspan="4" width="20" background="../images/index_02.gif">&nbsp;</td>
		<td width="175" height="53" style="table-layout:fixed>
			<a href="index.html"><a href="../index.html"><img src="../images/HomeGray.gif" alt="" width="175" height="53" border="0"></a></a></td>
		<td rowspan="4" width="65" background="../images/index_12.gif">&nbsp;</td>
		<td width="520" rowspan="4" valign="top"><p class="STYLE3">Adherent Raindrop Detection and Removal in Video</p>
		  <p>presented in <a href="http://www.pamitc.org/cvpr13/">CVPR 2013</a>, IEEE TPAMI 2015 </p>
		  <p><strong>Shaodi You</strong>, <a href="http://www.staff.science.uu.nl/~tan00109/">Robby T. Tan</a>, <a href="http://www.cvl.iis.u-tokyo.ac.jp/~rei/">Rei Kawakami</a>, <a href="http://omilab.naist.jp/~mukaigawa/" target="_blank">Yasuhiro Mukaigawa</a> and <a href="http://www.cvl.iis.u-tokyo.ac.jp/~ki/">Katsushi Ikeuchi</a></p>
		  <p><img src="../PubImage/CVPR2013-1.png" width="520" height="298"></p>
		  <p align="justify">Raindrops adhered to a windscreen or window glass can significantly 
		    degrade the visibility of a scene. 
		    Modeling, detecting and removing raindrops will, therefore, benefit many 
		    computer vision applications, particularly outdoor surveillance 
		    systems and intelligent vehicle systems.  
		    In this paper,  a method that automatically detects and removes 
		    adherent raindrops is introduced. 
		    The core idea is to exploit the local spatio-temporal derivatives of 
		    raindrops. 
		    To accomplish the idea, we first model adherent raindrops using law of physics, and detect raindrops based on these models in combination with motion and intensity temporal derivatives of the input video. 
		    Having detected the raindrops, we remove them and restore the images based on an analysis that some areas of raindrops 
		    completely occludes the scene, and some other areas occlude only  
		    partially.  
		    For partially occluding areas, we restore them  by retrieving as 
		    much as possible  information of the scene, namely, by solving a 
		    blending function on the detected partially occluding areas using 
		    the temporal intensity derivative. 
		    For completely occluding areas, we recover them by using a video 
		    completion technique.  
		    Experimental results using various real videos 
		    show the effectiveness of our method.</p>
		  <p><a href="../Downloads/ShaodiYou-PAMI.pdf">Journal paper</a> (12.7MB)<br>
              <a href="../Downloads/CVPR2013/ShaodiYou-CVPR2013.pdf">Conference paper</a> (2.5MB)<br>
              <a href="../CVPR2013/Shaodi_CVPR2013.html" target="_blank">Webpage</a><br>
          </p>
		  <p class="STYLE3">&nbsp;</p>
		  <p class="STYLE3">Raindrop Detection and Removal from Long Range Trajectory.</p>
		  <p>Oral presentation in ACCV 2014</p>
		  <p><strong>Shaodi You</strong>, <a href="http://php-robbytan.rhcloud.com/">Robby T. Tan</a>, <a href="http://www.cvl.iis.u-tokyo.ac.jp/~rei/">Rei Kawakami</a>, <a href="http://omilab.naist.jp/~mukaigawa/" target="_blank">Yasuhiro Mukaigawa</a> and <a href="http://www.cvl.iis.u-tokyo.ac.jp/~ki/">Katsushi Ikeuchi</a></p>
		  <p class="STYLE3"><img src="../PubImage/ACCV2014.png" width="520" height="312"></p>
		  <p align="justify">In rainy scenes, visibility can be degraded by raindrops which have adhered to the windscreen or camera lens. In order to resolve this degradation, we propose a method that automatically detects and removes adherent raindrops.  The idea is to use long range trajectories to discover the motion and appearance features of raindrops locally along the trajectories. These motion and appearance features are obtained through our analysis of the trajectory behavior when encountering raindrops. These features are then transformed into a labeling problem, which the cost function can be optimized efficiently. Having detected raindrops, the removal is achieved by utilizing patches indicated, enabling the motion consistency to be preserved. Our trajectory based video completion method not only removes the raindrops but also complete the motion field, which benefits motion estimation algorithms to possibly work in rainy scenes. Experimental results on real videos show the effectiveness of the proposed method. </p>
		  <p><a href="../ACCV2014/shaodi_accv14.pdf" target="_blank">pdf</a>(3.0MB) <br>
            <a href="../ACCV2014/Shaodi_ACCV2014.html">Webpage</a> </p>
		  <p class="STYLE3">&nbsp;</p>
		  <p class="STYLE3">Local Background Enclosure for RGB-D Salient Object Detection</p>
		  <p>Appeared in CVPR 2016, Spotlight Presentation </p>
		  <p>David Fung, Nick Barnes, <strong>Shaodi You</strong>,  Chris McCarthy, </p>
		  <p><img src="../PubImage/David.png" alt="RGB-D saliency" width="520" height="365"></p>
		  <p align="justify">Recent work in salient object detection has considered 
		    the incorporation of depth cues from RGB-D images. In 
		    most cases, absolute depth, or depth contrast is used as 
		    the main feature. However, regions of high contrast in 
		    background regions cause false positives for such methods, 
		    as the background frequently contains regions that are 
		    highly variable in depth. Here, we propose a novel RGB-D 
		    saliency feature. Local background enclosure captures the 
		    spread of angular directions which are background with respect 
		    to the candidate region and the object that it is part 
		    of. We show that our feature improves over state-of-the-art 
		    RGB-D saliency approaches as well as RGB methods on the 
		    RGBD1000 and NJUS2000 datasets.</p>
		  <p align="justify"><a href="../Downloads/David_Saliency_CVPR16.pdf">Paper</a></p>
		  <p class="STYLE3">&nbsp;</p>
		  <p class="STYLE3">Robust and Fast Motion Estimation for Video Completion.</p>
		  <p>Oral presentation in <a href="http://www.mva-org.jp/mva2013/">IAPR MVA  2013</a></p>
		  <p><strong>Shaodi You</strong>, <a href="http://www.staff.science.uu.nl/~tan00109/">Robby T. Tan</a>, <a href="http://www.cvl.iis.u-tokyo.ac.jp/~rei/">Rei Kawakami</a> and <a href="http://www.cvl.iis.u-tokyo.ac.jp/~ki/">Katsushi Ikeuchi</a></p>
		  <p><img src="../PubImage/MVA2013.png" width="520" height="280"></p>
		  <p align="justify">A motion estimation method for completing a video 
		    with large and consecutive damage is introduced. It 
		    is principally based on sparse matching and interpolation. First, SIFT, which is robust to arbitrary motion,<br>
		    is used to efficiently obtain sparse correspondences in 
		    neighboring frames. To ensure these correspondences 
		    are uniformly distributed across the image, a fast dense 
		    point sampling method is applied. Then, a dense motion field is generated by interpolating the correspondences. An efficient weighted explicit polynomial fitting 
		    method is proposed to achieve spatially and temporally 
		    coherent interpolation. In the experiment, quantitative 
		    measurements were conducted to show the robustness 
		    and effectiveness of the proposed method.</p>
		  <p><a href="../Downloads/ShaodiYou-MVA2013.pdf">Paper</a> (0.9MB)<br>
		    Video demo<br>
		    Slides <br>
		    Bibtex</p>
		  <p class="STYLE3">&nbsp;</p>
		  <p><span class="STYLE3">Manifold    Topological Multi-Resolution Analysis Method</span></p>
		  <p>Appeared in <a href="http://www.journals.elsevier.com/pattern-recognition/">Pattern Recognition</a></p>
		  <p><strong>Shaodi You</strong> and <a href="http://oa.ee.tsinghua.edu.cn/~mahuimin/index.htm">Huimin Ma</a> </p>
		  <p><img src="../PubImage/Pub2.jpg" width="520" height="303"></p>
		  <p align="justify">In this paper, two significant weaknesses of locally linear embedding (LLE) applied to computer vision 
		    are addressed: &quot;intrinsic dimension&quot; and &quot;eigenvector meanings&quot;. &quot;Topological embedding&quot; and 
		    &quot;multi-resolution nonlinearity capture&quot; are introduced based on mathematical analysis of topological 
		    manifolds and LLE. The manifold topological analysis (MTA) method is described and is based on 
		    &quot;topological embedding&quot;. MTA is a more robust method to determine the &quot;intrinsic dimension&quot; of a 
		    manifold with typical topology, which is important for tracking and perception understanding. The 
		    manifold multi-resolution analysis (MMA) method is based on &quot;multi-resolution nonlinearity capture&quot;. 
		    MMA defines LLE eigenvectors as features for pattern recognition and dimension reduction. Both MTA 
		    and MMA are proved mathematically, and several examples are provided. Applications in 3D object 
		    recognition and 3D object viewpoint space partitioning are also described.</p>
		  <p><a href="../Downloads/ShaodiYOU-PR.pdf">Paper</a> (3.8MB)</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p class="STYLE3">A solution for efficient viewpoint space  partition in 3D object recognition</p>
		  <p>Oral presentation in ICIG2009</p>
		  <p> Xiao Yu, <a href="http://oa.ee.tsinghua.edu.cn/~mahuimin/index.htm">Huimin Ma</a>, <strong>Shaodi You</strong> and Ze Yuan</p>
		  <p><img src="../PubImage/Pub3.jpg" width="520" height="309"></p>
		  <p align="justify">Viewpoint Space Partition based on Aspect Graph is one of the core techniques of 3D object recognition. Projection images obtained from critical viewpoint following this approach can efficiently provide topological information of an object. Computational complexity has been a huge challenge for obtaining the representation viewpoints used in 3D recognition. In this paper, we discuss inefficiency of calculation due to redundant nonexistent visual events; propose a systematic criterion for edge selection involved in EEE events. Pruning algorithm based on concave-convex property is demonstrated. We further introduce intersect relation into our pruning algorithm. These two methods not only enable the calculation of EEE events, but also can be implemented before viewpoint calculation, hence realizes view-independent pruning algorithm. Finally, analysis on simple representative models supports the effectiveness of our methods. Further investigations on Princeton Models, including airplane, automobile, etc, show a two orders of magnitude reduction in the number of EEE events on average.</p>
		  <p align="justify"><a href="../Downloads/ShaodiYOU-ICIG2009.pdf">Paper</a> (0.5MB)</p>
		  <p align="justify">&nbsp;</p>
		  <p align="justify"><span class="STYLE3">Single Image Stereo Using Water Drops</span></p>
		  <p>Submitted to SIGGRAPH  2016 </p>
		  <p><strong>Shaodi You</strong>, <a href="http://php-robbytan.rhcloud.com/">Robby T. Tan</a>, <a href="http://www.cvl.iis.u-tokyo.ac.jp/~rei/">Rei Kawakami</a>, <a href="http://omilab.naist.jp/~mukaigawa/" target="_blank">Yasuhiro Mukaigawa</a> and <a href="http://www.cvl.iis.u-tokyo.ac.jp/~ki/">Katsushi Ikeuchi</a></p>
		  <p><img src="../PubImage/Stereo.png" alt="Stereo" width="520" height="137"></p>
		  <p>This paper introduces depth estimation from water drops. The key idea is that a single water drop adhered to window glass is totally transparent and convex, and thus optically acts like a fisheye lens. If we have more than one water drop in a single image, then through each of them we can see the environment with different view points, similar to  stereo. To realize this idea, we need to rectify every water drop imagery to make radially distorted planar surfaces look flat. For this rectification, we consider two physical properties of water drops: (1)  A static water drop has constant volume, and its geometric convex shape is determined by the balance between the tension force and gravity. This implies that the 3D geometric shape can be obtained by minimizing  the overall potential energy, which is the sum of the tension energy and the gravitational potential energy. (2) The imagery inside a water-drop is determined by the water-drop 3D shape and total reflection at the boundary. This total reflection generates a dark band commonly observed in any adherent water drops. Hence, once the 3D shape of water drops are recovered, we can rectify the water drop images through backward raytracing. Subsequently, we can compute depth using stereo. In addition to depth estimation, we can also apply image refocusing. Experiments on real images and a quantitative evaluation show the effectiveness of our proposed method. To our best knowledge, never before have adherent water drops been used to estimate depth.</p>
		  <p><a href="../Downloads/ShaodiYOU_Stereo.pdf" target="_blank">pdf</a> (3.5MB)</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p><span class="STYLE3">Origami: Multi-view Rectification of Folded Documents </span></p>
	      <p><strong>Shaodi You</strong>, <a href="http://research.microsoft.com/en-us/people/yasumat/" target="_blank">Yasuyuki Matsushita</a>, <a href="http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CB4QFjAA&url=http%3A%2F%2Fresearch.microsoft.com%2Fjump%2F75451&ei=6sgGVa_zJobm8AXd-4CwDw&usg=AFQjCNE0wuiLZ7bvwl4BgzxNAyqDOyX1FQ&sig2=H-a-i_Sw3Vh-croB_8CWig" target="_blank">Sudipta Sinha</a>, Yusuke Bou and <a href="http://www.cvl.iis.u-tokyo.ac.jp/~ki/">Katsushi Ikeuchi</a></p>
	      <p><img src="../PubImage/Origami.png" width="520" height="270"></p>
	      <p align="justify">Digitally unwrapping paper sheets is a crucial step for document scanning and accurate text recognition. This paper presents a method for automatically rectifying curved or folded paper sheets from a small number of images captured from different viewpoints. Unlike previous techniques that require either an expensive 3D scanner or over-simplified parametric representation of the deformations, our method only uses a few images and is based on general developable surface model that can represent diverse sets of deformation of paper sheets. By exploiting the geometric property of developable surfaces, we develop a robust rectification method based on ridge-aware 3D reconstruction of the paper sheet and L1 conformal mapping. We evaluate the proposed technique quantitatively and qualitatively using a wide variety of input documents, such as receipts, book pages and letters. </p>
	      <p><a href="../Downloads/ShaodiYou_Origami.pdf">Paper</a> (3.0MB)</p>
	      <p>&nbsp;</p>
	      <p class="STYLE3">&nbsp;</p>
	      <p align="justify" class="STYLE3">Thing Locally, Fit Globally: 
	        Robust and Fast 3D Shape Matching 
          via Adaptive Algebraic Fitting</p>
	      <p align="justify"><strong>Shaodi You </strong></p>
	      <p align="justify" class="STYLE3"><img src="../PubImage/R1.jpg" alt="Algebraic" width="520" height="321"></p>
	      <p align="justify">we propose a novel 3D free form surface matching method based 
	        on a novel key-point detector and a novel feature descriptor. The proposed
	        detector is based on algebraic surface fitting. By global smooth fitting, our detector
	        achieved high computational efficiency and robustness against non-rigid
	        deformations. For the feature descriptor, we provide algorithms to compute
	        3D critical net which generates a meaningful structure on standalone local keypoints. 
	        The scale invariant and deformation robust Dual Spin Image descriptor 
	        is provided based on the 3D critical net. Our method is proved by solid 
	        mathematics. Intensive quantitative experiments demonstrate the robustness, 
          efficiency and accuracy of the proposed method.</p>
	      <p align="justify"><a href="../Downloads/ShaodiYOU_NeuralComp.pdf">Paper</a></p></td>
		<td rowspan="4">
			<img src="../images/index_06.gif" width="40" height="514" alt=""></td>
		<td rowspan="4" width="20" background="../images/index_07.gif">&nbsp;</td>
	</tr>
	<tr>
		<td width="175" height="53" style="table-layout:fixed>
			<a href="Html/CV.html"><p><a href="CV.html"><img src="../images/CVGray.gif" alt="" width="175" height="53" border="0"></a><a href="Research.html"><img src="../images/ResearchBK.gif" width="175" height="53" border="0"></a></a></p>	    </td>
	</tr>
	<tr>
		<td width="175" height="53">
			<a href="Publication.html"><img src="../images/PubGray.gif" alt="" width="175" height="53" border="0"></a><a href="Artworks.html"><img src="../images/ArtGray.gif" alt="Artworks" width="175" height="54" border="0"></a></td>
	</tr>
	<tr>
		<td><p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p></td>
	</tr>
	<tr>
		<td colspan="6">
			<img src="../images/index_11.gif" width="840" height="59" alt=""></td>
	</tr>
</table>
<!-- End Save for Web Slices -->
</body>
</html>