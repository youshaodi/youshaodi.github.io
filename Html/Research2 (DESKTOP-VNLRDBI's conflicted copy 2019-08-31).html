<html>
<head>
<title>Shaodi YOU - Publications</title>
<meta http-equiv="Content-Type" content="text/html; charset=Helvetica">

<style type="text/css">
p {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 16px;
}
.STYLE3 {font-size: 24px}
</style>
</head>
<body bgcolor="#FFFFFF" leftmargin="0" topmargin="0" marginwidth="0" marginheight="0">
<!-- Save for Web Slices (Untitled-1.psd) -->
<table width="840" height="768" border="0" align="center" cellpadding="0" cellspacing="0" id="Table_01">
	<tr>
		<td colspan="6">
			<img src="../images/index_01.gif" width="840" height="195" alt=""></td>
	</tr>
	<tr>
		<td rowspan="4" width="20" background="../images/index_02.gif">&nbsp;</td>
		<td width="175" height="53" style="table-layout:fixed>
			<a href="index.html"><a href="../index.html"><img src="../images/HomeGray.gif" alt="" width="175" height="53" border="0"></a></a></td>
		<td rowspan="4" width="65" background="../images/index_12.gif">&nbsp;</td>
		<td width="520" rowspan="4" valign="top"><p align="justify" class="STYLE3">Perception Based Vision</p>
		  <p align="justify">Perceptual based vision aiming to  model and understanding human perception. And later propose computer vision and  machine learning algorithms which incorporates such understanding. Specifically,  I have been working on the topics: Prothetic vision, decolourization, saliency  and manifold learning.</p>
		  <p class="STYLE3">&nbsp;</p>
		  <p class="STYLE3">Single Image Action Recognition using Semantic Body Part Actions</p>
		  <p>Appeared in ICCV 2017</p>
		  <p>Zhichen Zhao, Huimin Ma, <strong>Shaodi You</u> </strong>
		  <p class="STYLE3"><img src="../PubImage/ICCV2017.png" alt="ICCV2017" width="520" height="223"></p>
		  <p align="justify">In this paper, we propose a novel single image action 
		    recognition algorithm based on the idea of semantic part 
		    actions. Unlike existing part-based methods, we argue that 
		    there exists a mid-level semantic, the semantic part action; 
		    and human action is a combination of semantic part actions<br>
		    and context cues. In detail, we divide human body into 
		    seven parts: head, torso, arms, hands and lower body. For 
		    each of them, we define a few semantic part actions (e.g. 
		    head: laughing). Finally, we exploit these part actions to 
		    infer the entire body action (e.g. applauding). To make the<br>
		    proposed idea practical, we propose a deep network-based 
		    framework which consists of two subnetworks, one for part 
		    localization and the other for action prediction. The action 
		    prediction network jointly learns part-level and body-level 
		    action semantics and combines them for the final decision. 
		    Extensive experiments demonstrate our proposal on semantic 
		    part actions as elements for entire body action. Our 
		    method reaches mAP of 93.9% and 91.2% on PASCAL VOC 
		    2012 and Stanford-40, which outperforms the state-of-theart 
	      by 2.3% and 8.6%.</p>
		  <p><a href="../Downloads/ShaodiYOU_ICCV2017.pdf" target="_blank">Paper</a> (3.5MB) <br>
            <a href="https://github.com/ZhichenZhao/part-action-network" target="_blank">Webpage</a></p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p align="justify" class="STYLE3">Edge Preserving and Multi-Scale Contextual Neural 
	      Network for Salient Object Detection</p>
		  <p align="justify">Appeared in TIP 2017 </p>
		  <p align="justify">Xiang Wang, Huimin Ma, <strong>Shaodi You</strong> and Xiaozhi Chen </p>
		  <p><img src="../PubImage/TIP2017.png" alt="TIP2017" width="520" height="191"></p>
		  <p align="justify">We propose a novel edge preserving 
		    and multi-scale contextual neural network for salient object 
		    detection. The proposed framework is aiming to address two 
		    limits of the existing CNN based methods. First, region-based<br>
		    CNN methods lack sufficient context to accurately locate salient 
		    object since they deal with each region independently. Second, 
		    pixel-based CNN methods suffer from blurry boundaries due 
		    to the presence of convolutional and pooling layers. Motivated 
		    by these, we first propose an end-to-end edge-preserved neural 
		    network based on Fast R-CNN framework (named RegionNet) to 
		    efficiently generate saliency map with sharp object boundaries. 
		    Later, to further improve it, multi-scale spatial context is attached 
		    to RegionNet to consider the relationship between regions and the 
		    global scenes. Furthermore, our method can be generally applied 
		    to RGB-D saliency detection by depth refinement. The proposed<br>
		    framework achieves both clear detection boundary and multiscale 
		    contextual robustness simultaneously for the first time, and 
		    thus achieves an optimized performance. Experiments on six 
		    RGB and two RGB-D benchmark datasets demonstrate that the 
	      proposed method achieves state-of-the-art performance.</p>
		  <p align="justify"><a href="../Downloads/Xiang_TIP2017.pdf" target="_blank">pdf</a> (4.9MB) </p>
		  <p class="STYLE3">&nbsp;</p>
		  <p class="STYLE3">Local Background Enclosure for RGB-D Salient Object Detection</p>
		  <p>Appeared in CVPR 2016, Spotlight presentation </p>
		  <p>David Fung, Nick Barnes, <strong>Shaodi You</strong>,  Chris McCarthy, </p>
		  <p><img src="../PubImage/David.png" alt="RGB-D saliency" width="520" height="365"></p>
		  <p align="justify">Recent work in salient object detection has considered 
		    the incorporation of depth cues from RGB-D images. In 
		    most cases, absolute depth, or depth contrast is used as 
		    the main feature. However, regions of high contrast in 
		    background regions cause false positives for such methods, 
		    as the background frequently contains regions that are 
		    highly variable in depth. Here, we propose a novel RGB-D 
		    saliency feature. Local background enclosure captures the 
		    spread of angular directions which are background with respect 
		    to the candidate region and the object that it is part 
		    of. We show that our feature improves over state-of-the-art 
		    RGB-D saliency approaches as well as RGB methods on the 
		    RGBD1000 and NJUS2000 datasets.</p>
		  <p align="justify"><a href="../Downloads/David_Saliency_CVPR16.pdf">Paper</a><br>
	      <a href="http://users.cecs.anu.edu.au/~u4673113/lbe.html" target="_blank">Webpage</a></p>
		  <p class="STYLE3">&nbsp;</p>
		  <p class="STYLE3">Perceptually Consistent Color-to-Gray Image Conversion</p>
		  <p><strong>Shaodi You</strong>, Nick Barnes and Janine Walker </p>
		  <p class="STYLE3"><img src="../PubImage/C2G.png" alt="C2G" width="520" height="129"></p>
		  <p align="justify">We propose a color to grayscale image conversion algorithm (C2G) that aims to preserve the perceptual properties of the color image as much as possible. To this end, we propose measures for two perceptual properties based on contemporary research in vision science: brightness and multi-scale contrast. The brightness measurement is based on the idea that the brightness of a grayscale image will affect the perception of the probability of color information. The color contrast measurement is based on the idea that the contrast of a given pixel to its surroundings can be measured as a linear combination of color contrast at different scales. Based on these measures we propose a graph based optimization framework to balance the brightness and contrast measurements. To solve the optimization, an L1-norm based method is provided which converts color discontinuities to brightness discontinuities. To validate our methods, we evaluate against the existing cadik and Color250 datasets, and against NeoColor, a new dataset that improves over existing C2G datasets. NeoColor contains around 300 images from typical C2G scenarios, including: commercial photograph, printing, books, magazines, masterpiece artworks and computer designed graphics. We show improvements in metrics of performance, and further through a user study, we validate the performance of both the algorithm and the metric.	      </p>
		  <p><a href="../Downloads/ShaodiYOU_C2G.pdf">Paper</a></p>
		  <p class="STYLE3">&nbsp;</p>
		  <p class="STYLE3">Learning RGB-D Salient Object Detection using background enclosure, depth contrast, and top-down features</p>
		  <p>Appeared in ICCV Workshop on Mutual Benefit of Cognitive and Computer Vision, 2017</p>
		  <p>Riku Shigematsu, David Feng, <strong>Shaodi You</strong> and Nick Barnes </p>
		  <p class="STYLE3"><img src="../PubImage/MBCC2017.png" alt="MBCC2017" width="520" height="406"></p>
		  <p align="justify">In human visual saliency, top-down and bottom-up information are ombined as a basis of visual attention. Recently, deep Convolutional Neural etworks (CNN) have demonstrated strong performance on RGB salient object detection, providing an effective mechanism for combining top-down semantic information with low level features. Although depth information has been shown to be important for human perception of salient objects, the use of top-down information and the exploration of CNNs for RGB-D salient object detection remains limited. 
	      Here we propose a novel deep CNN architecture for RGB-D salient object detection that utilizes both top-down and bottom-up cues. In order to produce such an architecture, we present novel depth features that capture the ideas of background enclosure, depth contrast and histogram distance in a manner that is suitable for a learned approach. We show improved results compared to state-of-the-art RGB-D salient object detection methods. We also show that the low-level and mid-level depth features both contribute to improvements in results. In particular, the F-Score of our method is 0.848 on RGBD1000, which is 10.7% better than the current best.</p>
		  <p align="justify"><a href="../Downloads/ShaodiYOU_MBCC2017.pdf" target="_blank">Paper</a> (2.1MB) <br>
		  Webpage</p>
		  <p class="STYLE3">&nbsp;</p>
		  <p class="STYLE3">DSD: Depth Structural Descriptor for Edge-Based Assistive Navigation</p>
		  <p>Appeared in ICCV Workshop on Assistive Computer Vision and Robotics</p>
		  <p>David Feng, <strong>Shaodi You</strong> and Nick Barnes</p>
		  <p><img src="../PubImage/ACVR2017.png" alt="ACVR2017" width="520" height="337"></p>
		  <p align="justify">Structural edge detection is the task of finding edges between 
		    significant surfaces in a scene. This can underpin 
		    many computer vision tasks such as sketch recognition and 
		    3D scene understanding, and is important for conveying 
		    scene structure for navigation with assistive vision. Identifying 
		    structural edges from a depth image can be challenging 
		    because surface structure that differentiates edges is not 
		    well represented in this format. We derive a depth input encoding, 
		    the Depth Surface Descriptor (DSD), that captures 
		    the first order properties of surfaces, allowing for improved 
		    classification of surface geometry that corresponds to structural 
		    edges. We apply the DSD feature to salient edge detection 
		    on RGB-D images using a fully convolutional neural 
		    network with deep supervision. We evaluate our method 
		    on both a new RGB-D dataset containing prosthetic vision 
		    scenarios, and the SUNRGBD dataset, and show that our 
		    approach produces improved performance compared to existing 
		    methods by 4%.</p>
		  <p><a href="../Downloads/ShaodiYOU_ACVR2017.pdf" target="_blank">Paper</a> (10.2MB) <br>
	      Webpage</p>
		  <p class="STYLE3"></p>
		  <p class="STYLE3"></p>
		  <p class="STYLE3">HOSO: Histogram Of Surface Orientation for<br>
	      RGB-D Salient Object Detection</p>
		  <p>Appeared in The International Conference on Digital Image Computing: Techniques and  Applications, DICTA 2017</p>
		  <p>David Feng, Nick Barnes and <strong>Shaodi You </strong></p>
		  <p class="STYLE3"><img src="../PubImage/DICTA2017_2.png" alt="DICTA2017_2" width="520" height="219"></p>
		  <p align="justify">Salient object detection using RGB-D data is an 
		    emerging field in computer vision. Salient regions are often 
		    characterized by an unusual surface orientation profile with 
		    respect to the surroundings. To capture such profile, we introduce 
		    the histogram of surface orientation (HOSO) feature 
		    to measure surface orientation distribution contrast for RGB-D 
		    saliency. We propose a new unified model that integrates surface 
		    orientation distribution contrast with depth and color contrast 
		    across multiple scales. This model is implemented in a multistage 
		    saliency computation approach that performs contrast 
		    estimation using a kernel density estimator (KDE), estimates 
		    object positions from the low-level saliency map, and finally 
		    refines the estimated object positions with a graph cut based 
		    approach. Our method is evaluated on two RGB-D salient object 
		    detection databases, achieving superior performance to previous 
	      state-of-the-art methods.</p>
		  <p align="justify"><a href="../Downloads/DavidFENG_DICTA2017.pdf" target="_blank">Paper</a> (6.2MB) </p>
		  <p class="STYLE3"></p>
		  <p class="STYLE3"></p>
		  <p class="STYLE3"></p>
		  <p class="STYLE3">Double-Guided Filtering: Image Smoothing with Structure and Texture  Guidance</p>
		  <p>Appeared in The International Conference on Digital Image Computing: Techniques and  Applications, DICTA 2017</p>
		  <p>Kaiyue Lu, <strong>Shaodi You</strong>, Nick Barnes</p>
		  <p class="STYLE3"><img src="../PubImage/DICTA2017_1.png" alt="DICTA2017_1" width="520" height="293"></p>
		  <p align="justify">Image smoothing is a fundamental technology which aims to preserve image structure and remove insignificant texture. Balancing the trade-off between preserving structure and suppressing texture, however, is not a trivial task. 
	      This is because existing methods rely on only one guidance to infer structure or texture and assume the other is dependent. However, in many cases, textures are composed of repetitive structures and difficult to be distinguished by only one guidance. In this paper, we aim to better solve the trade-off by applying two independent guidances for structure and texture. Specifically, we adopt semantic edge detection as structure guidance, and texture decomposition as texture guidance. Based on this, we propose a kernel-based image smoothing method called the double-guided filter (DGF). In the paper, for the first time, we introduce the concept of texture guidance, and DGF, the first kernel-based method that leverages structure and texture guidance at the same time to be both 'structure-aware' and 'texture-aware'. We present a number of experiments to show the effectiveness of the proposed filter.</p>
		  <p align="justify"><a href="../Downloads/KaiyueLU_DICTA2017.pdf" target="_blank">Paper</a> (3.8 MB) </p>
		  <p class="STYLE3"></p>
		  <p class="STYLE3"></p>
		  <p><span class="STYLE3">Manifold    Topological Multi-Resolution Analysis Method</span></p>
		  <p>Appeared in <a href="http://www.journals.elsevier.com/pattern-recognition/">Pattern Recognition</a></p>
		  <p><strong>Shaodi You</strong> and <a href="http://oa.ee.tsinghua.edu.cn/~mahuimin/index.htm">Huimin Ma</a> </p>
		  <p><img src="../PubImage/Pub2.jpg" alt="Perception" width="520" height="303"></p>
		  <p align="justify">In this paper, two significant weaknesses of locally linear embedding (LLE) applied to computer vision 
		    are addressed: &quot;intrinsic dimension&quot; and &quot;eigenvector meanings&quot;. &quot;Topological embedding&quot; and 
		    &quot;multi-resolution nonlinearity capture&quot; are introduced based on mathematical analysis of topological 
		    manifolds and LLE. The manifold topological analysis (MTA) method is described and is based on 
		    &quot;topological embedding&quot;. MTA is a more robust method to determine the &quot;intrinsic dimension&quot; of a 
		    manifold with typical topology, which is important for tracking and perception understanding. The 
		    manifold multi-resolution analysis (MMA) method is based on &quot;multi-resolution nonlinearity capture&quot;. 
		    MMA defines LLE eigenvectors as features for pattern recognition and dimension reduction. Both MTA 
		    and MMA are proved mathematically, and several examples are provided. Applications in 3D object 
		    recognition and 3D object viewpoint space partitioning are also described.</p>
		  <p><a href="../Downloads/ShaodiYOU-PR.pdf">Paper</a> (3.8MB)</p>
		  <p>&nbsp;</p>
		  <p class="STYLE3"> Automatic Generation of Grounded Visual Questions</p>
		  <p>Appeared in IJCAI 2017</p>
		  <p>Shijie Zhang, Lizhen Qu, <strong>Shaodi You</strong>, Zhenglu Yang and Jiawan Zhang </p>
		  <p><img src="../PubImage/IJCAI2017.png" alt="IJCAI2017" width="520" height="230"></p>
		  <p align="justify">In this paper, we propose the first model to be able to generate diverse visually grounded questions given the same image. Visual question generation is an emerging topic which links textual questions with visual input. To the best of our knowledge, it lacks automatic methods to generate various and reasonable questions for the same visual input. So far, almost all the textual questions are generated manually, as well as the corresponding answers. To this end, we propose a system that automatically generates visually grounded questions. First, visual input is analyzed with deep caption model. Second, the captions along with VGG-16 features are used as input for our proposed question generator to generate visually grounded questions. Finally, to enable generating of versatile questions, a question type selection module is provided which selects reasonable question types and provide them as parameters for question generation. This is done using a hybrid LSTM with both visual and answer input. Our system is trained using VQA and Visual7W dataset and shows reasonable results on automatically generating of new visual questions. We also propose a quantitative metric for automatic evaluation of the question quality.</p>
		  <p><a href="../Downloads/Shijie_VQG_IJCAI2017.pdf">Paper</a> (1.4 MB) </p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p class="STYLE3">A solution for efficient viewpoint space  partition in 3D object recognition</p>
		  <p>Oral presentation in ICIG2009</p>
		  <p> Xiao Yu, <a href="http://oa.ee.tsinghua.edu.cn/~mahuimin/index.htm">Huimin Ma</a>, <strong>Shaodi You</strong> and Ze Yuan</p>
		  <p><img src="../PubImage/Pub3.jpg" alt="Perception" width="520" height="309"></p>
		  <p align="justify">Viewpoint Space Partition based on Aspect Graph is one of the core techniques of 3D object recognition. Projection images obtained from critical viewpoint following this approach can efficiently provide topological information of an object. Computational complexity has been a huge challenge for obtaining the representation viewpoints used in 3D recognition. In this paper, we discuss inefficiency of calculation due to redundant nonexistent visual events; propose a systematic criterion for edge selection involved in EEE events. Pruning algorithm based on concave-convex property is demonstrated. We further introduce intersect relation into our pruning algorithm. These two methods not only enable the calculation of EEE events, but also can be implemented before viewpoint calculation, hence realizes view-independent pruning algorithm. Finally, analysis on simple representative models supports the effectiveness of our methods. Further investigations on Princeton Models, including airplane, automobile, etc, show a two orders of magnitude reduction in the number of EEE events on average.</p>
		  <p align="justify"><a href="../Downloads/ShaodiYOU-ICIG2009.pdf">Paper</a> (0.5MB)</p>		  </td>
		<td rowspan="4">
			<img src="../images/index_06.gif" width="40" height="514" alt=""></td>
		<td rowspan="4" width="20" background="../images/index_07.gif">&nbsp;</td>
	</tr>
	<tr>
		<td width="175" height="53" style="table-layout:fixed>
			<a href="Html/CV.html"><p><a href="CV.html"><img src="../images/CVGray.gif" alt="" width="175" height="53" border="0"></a><a href="Research.html"><img src="../images/ResearchBK.gif" width="175" height="53" border="0"></a></a></p>	    </td>
	</tr>
	<tr>
		<td width="175" height="53">
			<a href="Publication.html"></a><a href="Artworks.html"><img src="../images/ArtGray.gif" alt="Artworks" width="175" height="54" border="0"></a></td>
	</tr>
	<tr>
		<td><p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
		  <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p>
	    <p>&nbsp;</p></td>
	</tr>
	<tr>
		<td colspan="6">
			<img src="../images/index_11.gif" width="840" height="59" alt=""></td>
	</tr>
</table>
<!-- End Save for Web Slices -->
</body>
</html>